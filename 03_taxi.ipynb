{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "def has_won(reward):\n",
    "    return reward == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Q-Table\n",
    "\n",
    "Create an $M \\times N$ matrix, with \n",
    "- $M$ = size of action space\n",
    "- $N$ = size of state space / observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 6, Observation Space: 500\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "observation_space_size = env.observation_space.n\n",
    "\n",
    "print(\"Action Space: {}, Observation Space: {}\".format(action_space_size, observation_space_size))\n",
    "\n",
    "qtable = np.zeros((observation_space_size, action_space_size))\n",
    "#print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "total_training_episodes = 10000\n",
    "total_play_episodes = 10000\n",
    "learning_rate = 0.8\n",
    "max_steps = 9999\n",
    "\n",
    "#discounting rate\n",
    "gamma = 0.95\n",
    "\n",
    "#exploration rate\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 1.0\n",
    "epsilon = max_epsilon\n",
    "\n",
    "#exponential decay rate for exploration probability\n",
    "decay_rate = 0.0001\n",
    "print(decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = False\n",
    "verbosity = 0 # 0=light, 1=medium, 2=heavy, 3=desperate housewifes\n",
    "log_every_n_episode = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode\t| Steps survived\t| Epsilon \t| Average Reward\t| Successful Episodes\n",
      "\u001b[1m\u001b[92m1000\t| 106\t\t\t| 0.9049\t| -669.1570\t\t| 249 (24.90%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m2000\t| 81\t\t\t| 0.8188\t| -541.9055\t\t| 1033 (51.65%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m3000\t| 30\t\t\t| 0.7409\t| -437.9863\t\t| 2004 (66.80%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m4000\t| 47\t\t\t| 0.6704\t| -363.0335\t\t| 3004 (75.10%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m5000\t| 33\t\t\t| 0.6066\t| -309.0734\t\t| 4004 (80.08%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m6000\t| 22\t\t\t| 0.5489\t| -268.2493\t\t| 5004 (83.40%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m7000\t| 14\t\t\t| 0.4966\t| -236.9654\t\t| 6004 (85.77%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m8000\t| 24\t\t\t| 0.4494\t| -212.0327\t\t| 7004 (87.55%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m9000\t| 21\t\t\t| 0.4066\t| -191.6902\t\t| 8004 (88.93%), SOLVED!!!\u001b[0m\n",
      "\u001b[1m\u001b[92m10000\t| 20\t\t\t| 0.3679\t| -174.8373\t\t| 9004 (90.04%), SOLVED!!!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_training_reward = 0\n",
    "successful_training_episodes = 0\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print(\"Episode\\t| Steps survived\\t| Epsilon \\t| Average Reward\\t| Successful Episodes\")\n",
    "\n",
    "for episode in range(total_training_episodes):\n",
    "    if (verbosity > 1):\n",
    "        print(\"===================================\")\n",
    "        print(\"Starting Episode {}\".format(episode))\n",
    "        \n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    while (not done and step < max_steps):        \n",
    "        # step 1: choose action (explore or exploit)\n",
    "        explore = random.random() < epsilon\n",
    "\n",
    "        # step 2: take action\n",
    "        action = env.action_space.sample() if explore else np.argmax(qtable[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #if (custom_rewards):\n",
    "        #    reward = rewards[new_state]\n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        if (verbosity > 2):\n",
    "            print(\"Step: {}, Epsilon: {}, Observation: {}, Reward: {}\".format(new_state, epsilon, new_state, reward))\n",
    "        \n",
    "        if (render):\n",
    "            env.render()\n",
    "            \n",
    "        # step 3: update q-table\n",
    "        # Q'(s, a) := Q(s, a) + lr * (R(s, a) + gamma * max(Q(s', a')) - Q(s, a))\n",
    "        qtable[state][action] = qtable[state][action] + learning_rate * (reward + gamma * max(qtable[new_state]) - qtable[state, action])\n",
    "            \n",
    "        if (verbosity > 2):\n",
    "            print(qtable)\n",
    "        \n",
    "        state = new_state\n",
    "        step += 1\n",
    "    \n",
    "    total_training_reward += total_reward\n",
    "    \n",
    "    if (has_won(reward)):\n",
    "        successful_training_episodes += 1\n",
    "    \n",
    "    if (verbosity > 1 or (episode + 1) % log_every_n_episode == 0 or (verbosity > 0 and has_won(reward))):\n",
    "        print(\"{}{}\\t| {}\\t\\t\\t| {:1.4f}\\t| {:1.4f}\\t\\t| {} ({:3.2f}%){}\".format(\"\\033[1m\\033[92m\" if has_won(reward) else \"\", episode + 1, step - 1, epsilon, total_training_reward / (episode + 1), successful_training_episodes, successful_training_episodes / (episode + 1) * 100, \", SOLVED!!!\\033[0m\" if has_won(reward) else \"\"))\n",
    "    \n",
    "    if (verbosity > 0):\n",
    "        print(\"Q-Table:\")\n",
    "        print(qtable)\n",
    "        print(\"Board on GameOver:\")\n",
    "        env.render()\n",
    "    \n",
    "    epsilon = np.clip((1 - decay_rate) * epsilon, min_epsilon, max_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Episodes: 9004, Success Rate: 0.9004\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 5.20997639  6.53681725  5.20997639  6.53681725  7.93349184 -2.46318275]\n",
      " [ 7.93349184  9.40367562  7.93349184  9.40367562 10.9512375   0.40367562]\n",
      " ...\n",
      " [10.9512375  12.58025    10.9512375   9.40367562  1.9512375   1.9512375 ]\n",
      " [ 5.20997639  6.53681725  5.20997639  6.53681725 -3.79002361 -3.79002361]\n",
      " [16.1        14.295      16.1        18.          7.1         7.1       ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Successful Episodes: {}, Success Rate: {}\".format(successful_training_episodes, successful_training_episodes / total_training_episodes))\n",
    "print(qtable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the Agent Play the Game with our Q-Table now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Episodes: 10000, Success Rate: 100.00%, Most Consecutive Successes: 10000\n"
     ]
    }
   ],
   "source": [
    "successful_episodes = 0\n",
    "consecutive_successes = 0\n",
    "consecutive_successes_record = 0\n",
    "\n",
    "for episode in range(total_play_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while (not done):\n",
    "        action = np.argmax(qtable[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        step += 1\n",
    "    \n",
    "    if (verbosity > 0):\n",
    "        print(\"\\n{}Episode finished after {} steps. {}\".format(\"\\033[1m\\033[92m\" if has_won(reward) else \"\", step, \"SOLVED!!!\\033[0m\" if has_won(reward) else \"\"))\n",
    "        env.render()\n",
    "    \n",
    "    if (has_won(reward)):\n",
    "        successful_episodes += 1\n",
    "        consecutive_successes += 1\n",
    "        \n",
    "        if (consecutive_successes > consecutive_successes_record):\n",
    "            consecutive_successes_record = consecutive_successes\n",
    "    else:\n",
    "        consecutive_successes = 0\n",
    "\n",
    "print(\"Successful Episodes: {}, Success Rate: {:3.2f}%, Most Consecutive Successes: {}\".format(successful_episodes, successful_episodes / total_play_episodes * 100, consecutive_successes_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successfull Q-Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
