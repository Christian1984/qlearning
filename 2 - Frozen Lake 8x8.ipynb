{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "def has_won(state):\n",
    "    return state == 63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Q-Table\n",
    "\n",
    "Create an $M \\times N$ matrix, with \n",
    "- $M$ = size of action space\n",
    "- $N$ = size of state space / observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "observation_space_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((observation_space_size, action_space_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Rewards: Table to Map Custom Rewards to State and Feature-Toggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = [0, 1, 1, 1, 1, -10, 1, -10, 1, 1, 1, -10, -10, 1, 1, 100]\n",
    "# custom_rewards = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "total_training_episodes = 1000000\n",
    "total_play_episodes = 10000\n",
    "learning_rate = 0.8\n",
    "max_steps = 9999\n",
    "\n",
    "#discounting rate\n",
    "gamma = 0.95\n",
    "\n",
    "#exploration rate\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "\n",
    "#exponential decay rate for exploration probability\n",
    "decay_rate = 10 / total_training_episodes\n",
    "print(decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = False\n",
    "verbosity = 0 # 0=light, 1=medium, 2=heavy, 3=desperate housewifes\n",
    "log_every_n_episode = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Around With the Environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# \n",
    "# for step in range(200):\n",
    "#     observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#     \n",
    "#     print(\"================\")\n",
    "#     print(\"Step {}\".format(step))\n",
    "#     print(info['prob'])\n",
    "#     print(\"================\")\n",
    "#     \n",
    "#     if (render):\n",
    "#         env.render()\n",
    "#     \n",
    "#     if (done):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for numpy etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = [3, 5, 2]\n",
    "#print(np.argmax(a))\n",
    "\n",
    "#b = [[5, 2, 1], [2, 9, 1], [1, 0, -5]]\n",
    "#print(np.argmax(b[0]))\n",
    "\n",
    "#text coloring from https://stackoverflow.com/questions/287871/how-to-print-colored-text-in-terminal-in-python\n",
    "#class bcolors:\n",
    "#    HEADER = '\\033[95m'\n",
    "#    OKBLUE = '\\033[94m'\n",
    "#    OKGREEN = '\\033[92m'\n",
    "#    WARNING = '\\033[93m'\n",
    "#    FAIL = '\\033[91m'\n",
    "#    ENDC = '\\033[0m'\n",
    "#    BOLD = '\\033[1m'\n",
    "#    UNDERLINE = '\\033[4m'\n",
    "\n",
    "# print(\"\\033[95mHEADER\\033[0m\")\n",
    "# print(\"\\033[94mBLUE\\033[0m\")\n",
    "# print(\"\\033[92mGREEN\\033[0m\")\n",
    "# print(\"\\033[91mFAIL\\033[0m\")\n",
    "# print(\"\\033[1mBOLD\\033[0m\")\n",
    "# print(\"\\033[1m\\033[91mBOLD AND RED\\033[0m normal\")\n",
    "# print(\"\\033[1m\\033[92mBOLD AND GREEN\\033[0m normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000 survived for 48 steps. Epsilon: 0.9048\n",
      "Total Reward (Training): 14.0, Average Reward (Training): 0.0014, Successful Episodes: 14 (0.14%)\n",
      "\n",
      "Episode 20000 survived for 62 steps. Epsilon: 0.8187\n",
      "Total Reward (Training): 51.0, Average Reward (Training): 0.0026, Successful Episodes: 51 (0.26%)\n",
      "\n",
      "Episode 30000 survived for 42 steps. Epsilon: 0.7408\n",
      "Total Reward (Training): 93.0, Average Reward (Training): 0.0031, Successful Episodes: 93 (0.31%)\n",
      "\n",
      "Episode 40000 survived for 31 steps. Epsilon: 0.6703\n",
      "Total Reward (Training): 154.0, Average Reward (Training): 0.0039, Successful Episodes: 154 (0.39%)\n",
      "\n",
      "Episode 50000 survived for 4 steps. Epsilon: 0.6065\n",
      "Total Reward (Training): 222.0, Average Reward (Training): 0.0044, Successful Episodes: 222 (0.44%)\n",
      "\n",
      "Episode 60000 survived for 16 steps. Epsilon: 0.5488\n",
      "Total Reward (Training): 317.0, Average Reward (Training): 0.0053, Successful Episodes: 317 (0.53%)\n",
      "\n",
      "Episode 70000 survived for 78 steps. Epsilon: 0.4966\n",
      "Total Reward (Training): 404.0, Average Reward (Training): 0.0058, Successful Episodes: 404 (0.58%)\n",
      "\n",
      "Episode 80000 survived for 6 steps. Epsilon: 0.4493\n",
      "Total Reward (Training): 525.0, Average Reward (Training): 0.0066, Successful Episodes: 525 (0.66%)\n",
      "\n",
      "Episode 90000 survived for 91 steps. Epsilon: 0.4066\n",
      "Total Reward (Training): 667.0, Average Reward (Training): 0.0074, Successful Episodes: 667 (0.74%)\n",
      "\n",
      "Episode 100000 survived for 21 steps. Epsilon: 0.3679\n",
      "Total Reward (Training): 868.0, Average Reward (Training): 0.0087, Successful Episodes: 868 (0.87%)\n",
      "\n",
      "Episode 110000 survived for 29 steps. Epsilon: 0.3329\n",
      "Total Reward (Training): 1119.0, Average Reward (Training): 0.0102, Successful Episodes: 1119 (1.02%)\n",
      "\n",
      "Episode 120000 survived for 6 steps. Epsilon: 0.3012\n",
      "Total Reward (Training): 1394.0, Average Reward (Training): 0.0116, Successful Episodes: 1394 (1.16%)\n",
      "\n",
      "Episode 130000 survived for 28 steps. Epsilon: 0.2725\n",
      "Total Reward (Training): 1726.0, Average Reward (Training): 0.0133, Successful Episodes: 1726 (1.33%)\n",
      "\n",
      "Episode 140000 survived for 67 steps. Epsilon: 0.2466\n",
      "Total Reward (Training): 2129.0, Average Reward (Training): 0.0152, Successful Episodes: 2129 (1.52%)\n",
      "\n",
      "Episode 150000 survived for 135 steps. Epsilon: 0.2231\n",
      "Total Reward (Training): 2564.0, Average Reward (Training): 0.0171, Successful Episodes: 2564 (1.71%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 160000 survived for 126 steps. Epsilon: 0.2019\n",
      "Total Reward (Training): 3105.0, Average Reward (Training): 0.0194, Successful Episodes: 3105 (1.94%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 170000 survived for 18 steps. Epsilon: 0.1827\n",
      "Total Reward (Training): 3701.0, Average Reward (Training): 0.0218, Successful Episodes: 3701 (2.18%)\n",
      "\n",
      "Episode 180000 survived for 112 steps. Epsilon: 0.1653\n",
      "Total Reward (Training): 4419.0, Average Reward (Training): 0.0245, Successful Episodes: 4419 (2.46%)\n",
      "\n",
      "Episode 190000 survived for 96 steps. Epsilon: 0.1496\n",
      "Total Reward (Training): 5187.0, Average Reward (Training): 0.0273, Successful Episodes: 5187 (2.73%)\n",
      "\n",
      "Episode 200000 survived for 109 steps. Epsilon: 0.1353\n",
      "Total Reward (Training): 6095.0, Average Reward (Training): 0.0305, Successful Episodes: 6095 (3.05%)\n",
      "\n",
      "Episode 210000 survived for 62 steps. Epsilon: 0.1225\n",
      "Total Reward (Training): 7164.0, Average Reward (Training): 0.0341, Successful Episodes: 7164 (3.41%)\n",
      "\n",
      "Episode 220000 survived for 19 steps. Epsilon: 0.1108\n",
      "Total Reward (Training): 8325.0, Average Reward (Training): 0.0378, Successful Episodes: 8325 (3.78%)\n",
      "\n",
      "Episode 230000 survived for 28 steps. Epsilon: 0.1003\n",
      "Total Reward (Training): 9568.0, Average Reward (Training): 0.0416, Successful Episodes: 9568 (4.16%)\n",
      "\n",
      "Episode 240000 survived for 4 steps. Epsilon: 0.0907\n",
      "Total Reward (Training): 11020.0, Average Reward (Training): 0.0459, Successful Episodes: 11020 (4.59%)\n",
      "\n",
      "Episode 250000 survived for 144 steps. Epsilon: 0.0821\n",
      "Total Reward (Training): 12536.0, Average Reward (Training): 0.0501, Successful Episodes: 12536 (5.01%)\n",
      "\n",
      "Episode 260000 survived for 16 steps. Epsilon: 0.0743\n",
      "Total Reward (Training): 14161.0, Average Reward (Training): 0.0545, Successful Episodes: 14161 (5.45%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 270000 survived for 85 steps. Epsilon: 0.0672\n",
      "Total Reward (Training): 15992.0, Average Reward (Training): 0.0592, Successful Episodes: 15992 (5.92%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 280000 survived for 48 steps. Epsilon: 0.0608\n",
      "Total Reward (Training): 17980.0, Average Reward (Training): 0.0642, Successful Episodes: 17980 (6.42%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 290000 survived for 14 steps. Epsilon: 0.0550\n",
      "Total Reward (Training): 20134.0, Average Reward (Training): 0.0694, Successful Episodes: 20134 (6.94%)\n",
      "\n",
      "Episode 300000 survived for 199 steps. Epsilon: 0.0498\n",
      "Total Reward (Training): 22463.0, Average Reward (Training): 0.0749, Successful Episodes: 22463 (7.49%)\n",
      "\n",
      "Episode 310000 survived for 14 steps. Epsilon: 0.0450\n",
      "Total Reward (Training): 24999.0, Average Reward (Training): 0.0806, Successful Episodes: 24999 (8.06%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 320000 survived for 87 steps. Epsilon: 0.0408\n",
      "Total Reward (Training): 27607.0, Average Reward (Training): 0.0863, Successful Episodes: 27607 (8.63%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 330000 survived for 115 steps. Epsilon: 0.0369\n",
      "Total Reward (Training): 30476.0, Average Reward (Training): 0.0924, Successful Episodes: 30476 (9.24%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 340000 survived for 86 steps. Epsilon: 0.0334\n",
      "Total Reward (Training): 33577.0, Average Reward (Training): 0.0988, Successful Episodes: 33577 (9.88%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 350000 survived for 66 steps. Epsilon: 0.0302\n",
      "Total Reward (Training): 36829.0, Average Reward (Training): 0.1052, Successful Episodes: 36829 (10.52%)\n",
      "\n",
      "Episode 360000 survived for 163 steps. Epsilon: 0.0273\n",
      "Total Reward (Training): 40041.0, Average Reward (Training): 0.1112, Successful Episodes: 40041 (11.12%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 370000 survived for 72 steps. Epsilon: 0.0247\n",
      "Total Reward (Training): 43554.0, Average Reward (Training): 0.1177, Successful Episodes: 43554 (11.77%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 380000 survived for 57 steps. Epsilon: 0.0224\n",
      "Total Reward (Training): 47383.0, Average Reward (Training): 0.1247, Successful Episodes: 47383 (12.47%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 390000 survived for 40 steps. Epsilon: 0.0202\n",
      "Total Reward (Training): 51144.0, Average Reward (Training): 0.1311, Successful Episodes: 51144 (13.11%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 400000 survived for 83 steps. Epsilon: 0.0183\n",
      "Total Reward (Training): 55046.0, Average Reward (Training): 0.1376, Successful Episodes: 55046 (13.76%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 410000 survived for 140 steps. Epsilon: 0.0166\n",
      "Total Reward (Training): 59128.0, Average Reward (Training): 0.1442, Successful Episodes: 59128 (14.42%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 420000 survived for 153 steps. Epsilon: 0.0150\n",
      "Total Reward (Training): 63372.0, Average Reward (Training): 0.1509, Successful Episodes: 63372 (15.09%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 430000 survived for 26 steps. Epsilon: 0.0136\n",
      "Total Reward (Training): 67746.0, Average Reward (Training): 0.1575, Successful Episodes: 67746 (15.75%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 440000 survived for 76 steps. Epsilon: 0.0123\n",
      "Total Reward (Training): 72318.0, Average Reward (Training): 0.1644, Successful Episodes: 72318 (16.44%)\n",
      "\n",
      "Episode 450000 survived for 37 steps. Epsilon: 0.0111\n",
      "Total Reward (Training): 76969.0, Average Reward (Training): 0.1710, Successful Episodes: 76969 (17.10%)\n",
      "\n",
      "Episode 460000 survived for 141 steps. Epsilon: 0.0101\n",
      "Total Reward (Training): 81906.0, Average Reward (Training): 0.1781, Successful Episodes: 81906 (17.81%)\n",
      "\n",
      "Episode 470000 survived for 23 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 86777.0, Average Reward (Training): 0.1846, Successful Episodes: 86777 (18.46%)\n",
      "\n",
      "Episode 480000 survived for 143 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 91577.0, Average Reward (Training): 0.1908, Successful Episodes: 91577 (19.08%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 490000 survived for 175 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 96595.0, Average Reward (Training): 0.1971, Successful Episodes: 96595 (19.71%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 500000 survived for 139 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 101422.0, Average Reward (Training): 0.2028, Successful Episodes: 101422 (20.28%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 510000 survived for 152 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 106394.0, Average Reward (Training): 0.2086, Successful Episodes: 106394 (20.86%), SOLVED!!!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mEpisode 520000 survived for 82 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 111255.0, Average Reward (Training): 0.2140, Successful Episodes: 111255 (21.40%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 530000 survived for 89 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 116059.0, Average Reward (Training): 0.2190, Successful Episodes: 116059 (21.90%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 540000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 120917.0, Average Reward (Training): 0.2239, Successful Episodes: 120917 (22.39%)\n",
      "\n",
      "Episode 550000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 125794.0, Average Reward (Training): 0.2287, Successful Episodes: 125794 (22.87%)\n",
      "\n",
      "Episode 560000 survived for 64 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 130519.0, Average Reward (Training): 0.2331, Successful Episodes: 130519 (23.31%)\n",
      "\n",
      "Episode 570000 survived for 72 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 135441.0, Average Reward (Training): 0.2376, Successful Episodes: 135441 (23.76%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 580000 survived for 68 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 140285.0, Average Reward (Training): 0.2419, Successful Episodes: 140285 (24.19%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 590000 survived for 53 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 145216.0, Average Reward (Training): 0.2461, Successful Episodes: 145216 (24.61%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 600000 survived for 113 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 150196.0, Average Reward (Training): 0.2503, Successful Episodes: 150196 (25.03%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 610000 survived for 96 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 154859.0, Average Reward (Training): 0.2539, Successful Episodes: 154859 (25.39%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 620000 survived for 163 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 159747.0, Average Reward (Training): 0.2577, Successful Episodes: 159747 (25.77%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 630000 survived for 161 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 164643.0, Average Reward (Training): 0.2613, Successful Episodes: 164643 (26.13%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 640000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 169640.0, Average Reward (Training): 0.2651, Successful Episodes: 169640 (26.51%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 650000 survived for 101 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 174449.0, Average Reward (Training): 0.2684, Successful Episodes: 174449 (26.84%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 660000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 179236.0, Average Reward (Training): 0.2716, Successful Episodes: 179236 (27.16%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 670000 survived for 72 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 184096.0, Average Reward (Training): 0.2748, Successful Episodes: 184096 (27.48%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 680000 survived for 55 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 189000.0, Average Reward (Training): 0.2779, Successful Episodes: 189000 (27.79%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 690000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 193901.0, Average Reward (Training): 0.2810, Successful Episodes: 193901 (28.10%)\n",
      "\n",
      "Episode 700000 survived for 64 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 198745.0, Average Reward (Training): 0.2839, Successful Episodes: 198745 (28.39%)\n",
      "\n",
      "Episode 710000 survived for 132 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 203589.0, Average Reward (Training): 0.2867, Successful Episodes: 203589 (28.67%)\n",
      "\n",
      "Episode 720000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 208354.0, Average Reward (Training): 0.2894, Successful Episodes: 208354 (28.94%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 730000 survived for 80 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 213280.0, Average Reward (Training): 0.2922, Successful Episodes: 213280 (29.22%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 740000 survived for 76 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 218111.0, Average Reward (Training): 0.2947, Successful Episodes: 218111 (29.47%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 750000 survived for 145 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 223002.0, Average Reward (Training): 0.2973, Successful Episodes: 223002 (29.73%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 760000 survived for 47 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 227904.0, Average Reward (Training): 0.2999, Successful Episodes: 227904 (29.99%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 770000 survived for 111 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 232621.0, Average Reward (Training): 0.3021, Successful Episodes: 232621 (30.21%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 780000 survived for 110 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 237481.0, Average Reward (Training): 0.3045, Successful Episodes: 237481 (30.45%)\n",
      "\n",
      "Episode 790000 survived for 34 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 242404.0, Average Reward (Training): 0.3068, Successful Episodes: 242404 (30.68%)\n",
      "\n",
      "Episode 800000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 247404.0, Average Reward (Training): 0.3093, Successful Episodes: 247404 (30.93%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 810000 survived for 158 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 252316.0, Average Reward (Training): 0.3115, Successful Episodes: 252316 (31.15%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 820000 survived for 122 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 257399.0, Average Reward (Training): 0.3139, Successful Episodes: 257399 (31.39%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 830000 survived for 34 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 262341.0, Average Reward (Training): 0.3161, Successful Episodes: 262341 (31.61%)\n",
      "\n",
      "Episode 840000 survived for 28 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 267242.0, Average Reward (Training): 0.3181, Successful Episodes: 267242 (31.81%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 850000 survived for 103 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 272136.0, Average Reward (Training): 0.3202, Successful Episodes: 272136 (32.02%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 860000 survived for 72 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 276931.0, Average Reward (Training): 0.3220, Successful Episodes: 276931 (32.20%)\n",
      "\n",
      "Episode 870000 survived for 105 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 281738.0, Average Reward (Training): 0.3238, Successful Episodes: 281738 (32.38%)\n",
      "\n",
      "Episode 880000 survived for 199 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 286664.0, Average Reward (Training): 0.3258, Successful Episodes: 286664 (32.58%)\n",
      "\n",
      "Episode 890000 survived for 123 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 291629.0, Average Reward (Training): 0.3277, Successful Episodes: 291629 (32.77%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 900000 survived for 16 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 296501.0, Average Reward (Training): 0.3294, Successful Episodes: 296501 (32.94%), SOLVED!!!\u001b[0m\n",
      "\n",
      "Episode 910000 survived for 92 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 301482.0, Average Reward (Training): 0.3313, Successful Episodes: 301482 (33.13%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 920000 survived for 54 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 306574.0, Average Reward (Training): 0.3332, Successful Episodes: 306574 (33.32%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 930000 survived for 48 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 311476.0, Average Reward (Training): 0.3349, Successful Episodes: 311476 (33.49%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 940000 survived for 81 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 316342.0, Average Reward (Training): 0.3365, Successful Episodes: 316342 (33.65%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 950000 survived for 179 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 321144.0, Average Reward (Training): 0.3380, Successful Episodes: 321144 (33.80%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 960000 survived for 81 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 326080.0, Average Reward (Training): 0.3397, Successful Episodes: 326080 (33.97%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 970000 survived for 50 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 330831.0, Average Reward (Training): 0.3411, Successful Episodes: 330831 (34.11%), SOLVED!!!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 980000 survived for 164 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 335861.0, Average Reward (Training): 0.3427, Successful Episodes: 335861 (34.27%), SOLVED!!!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 990000 survived for 35 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 340662.0, Average Reward (Training): 0.3441, Successful Episodes: 340662 (34.41%)\n",
      "\n",
      "\u001b[1m\u001b[92mEpisode 1000000 survived for 29 steps. Epsilon: 0.0100\n",
      "Total Reward (Training): 345706.0, Average Reward (Training): 0.3457, Successful Episodes: 345706 (34.57%), SOLVED!!!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_training_reward = 0\n",
    "successful_training_episodes = 0\n",
    "epsilon = max_epsilon\n",
    "\n",
    "for episode in range(total_training_episodes):\n",
    "    if (verbosity > 1):\n",
    "        print(\"===================================\")\n",
    "        print(\"Starting Episode {}\".format(episode))\n",
    "        \n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    info = {'prob': 0}\n",
    "\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    while (not done and step < max_steps):        \n",
    "        # step 1: choose action (explore or exploit)\n",
    "        explore = random.random() < epsilon\n",
    "\n",
    "        # step 2: take action\n",
    "        action = env.action_space.sample() if explore else np.argmax(qtable[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #if (custom_rewards):\n",
    "        #    reward = rewards[new_state]\n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        if (verbosity > 2):\n",
    "            print(\"Step: {}, Epsilon: {}, Observation: {}, Reward: {}\".format(new_state, epsilon, new_state, reward))\n",
    "        \n",
    "        if (render):\n",
    "            env.render()\n",
    "            \n",
    "        # step 3: update q-table\n",
    "        # Q'(s, a) := Q(s, a) + lr * (R(s, a) + gamma * max(Q(s', a')) - Q(s, a))\n",
    "        qtable[state][action] = qtable[state][action] + learning_rate * (reward + gamma * max(qtable[new_state]) - qtable[state, action])\n",
    "            \n",
    "        if (verbosity > 2):\n",
    "            print(qtable)\n",
    "        \n",
    "        state = new_state\n",
    "        step += 1\n",
    "    \n",
    "    total_training_reward += total_reward\n",
    "    \n",
    "    if (has_won(state)):\n",
    "        successful_training_episodes += 1\n",
    "    \n",
    "    if (verbosity > 1 or (episode + 1) % log_every_n_episode == 0 or (verbosity > 0 and has_won(state))):\n",
    "        print(\"{}Episode {} survived for {} steps. Epsilon: {:1.4f}\\nTotal Reward (Training): {}, Average Reward (Training): {:1.4f}, Successful Episodes: {} ({:3.2f}%){}\\n\".format(\"\\033[1m\\033[92m\" if has_won(state) else \"\", episode + 1, step - 1, epsilon, total_training_reward, total_training_reward / (episode + 1), successful_training_episodes, successful_training_episodes / (episode + 1) * 100, \", SOLVED!!!\\033[0m\" if has_won(state) else \"\"))\n",
    "    \n",
    "    if (verbosity > 1):\n",
    "        print(\"Q-Table:\")\n",
    "        print(qtable)\n",
    "        print(\"Board on GameOver:\")\n",
    "        env.render()\n",
    "    \n",
    "    epsilon = np.clip((1 - decay_rate) * epsilon, min_epsilon, max_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Episodes: 345706, Success Rate: 0.345706\n",
      "[[5.76498699e-03 9.70163089e-03 1.62655486e-02 5.66304261e-03]\n",
      " [1.16223817e-02 2.61146813e-02 1.00721304e-02 6.37665782e-03]\n",
      " [8.97511691e-03 1.82060253e-02 3.47599532e-02 9.29257308e-03]\n",
      " [1.11310889e-02 1.18589341e-02 2.07529819e-02 3.23367402e-02]\n",
      " [1.72470807e-02 1.59724296e-02 2.53285143e-02 4.19562732e-02]\n",
      " [2.63834150e-02 3.47593944e-02 2.53354932e-02 2.70747416e-02]\n",
      " [3.38971891e-02 3.29603028e-02 4.92123608e-02 3.36337744e-02]\n",
      " [4.84155722e-02 5.79689127e-02 5.01745798e-02 4.95567342e-02]\n",
      " [5.66023487e-03 5.60734755e-03 4.07252290e-03 1.77960458e-02]\n",
      " [6.10211959e-03 6.15060590e-03 5.58481948e-03 1.70644988e-02]\n",
      " [5.64690398e-03 5.16429339e-03 6.77687689e-03 2.05762939e-02]\n",
      " [7.02845062e-03 1.00155614e-02 4.79993279e-03 1.95503264e-02]\n",
      " [1.60214958e-02 1.25474607e-02 1.59353990e-02 2.59481860e-02]\n",
      " [2.46294817e-02 6.97164467e-02 2.55594518e-02 1.93080789e-02]\n",
      " [3.58512353e-02 3.75766833e-02 8.02348857e-02 3.65840372e-02]\n",
      " [1.15244899e-01 3.63940354e-02 5.44403199e-02 3.67576796e-02]\n",
      " [3.53462558e-03 3.22475197e-03 2.98489193e-03 4.38294228e-03]\n",
      " [3.01371339e-03 3.17129861e-03 3.33195880e-03 5.44205754e-03]\n",
      " [4.81088436e-03 9.25587003e-04 6.77031787e-04 6.59580573e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.40045213e-03 3.82360045e-03 2.60902226e-02 1.72258290e-04]\n",
      " [1.13781937e-03 1.57014253e-03 7.35313709e-03 3.56582997e-02]\n",
      " [3.56786755e-02 3.77362287e-02 6.60865768e-02 3.72910797e-02]\n",
      " [4.40126459e-02 8.88491837e-02 6.33435774e-02 4.49463309e-02]\n",
      " [1.73554573e-03 1.79479245e-03 2.48291675e-03 1.00476075e-03]\n",
      " [4.40840909e-04 3.68297953e-03 6.00557409e-04 6.59834824e-04]\n",
      " [4.69685354e-04 4.59472593e-03 6.25315751e-04 1.22462327e-03]\n",
      " [2.75393306e-06 3.46261427e-06 1.66022919e-06 2.58703790e-03]\n",
      " [3.17529466e-02 4.12054720e-05 2.17299988e-05 7.66934917e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.26154103e-03 2.26485864e-03 6.19810000e-02 2.29180010e-02]\n",
      " [3.66497623e-02 7.83649195e-02 5.38858459e-02 4.94581758e-02]\n",
      " [9.59420454e-04 8.64504422e-04 1.10115783e-03 7.72985381e-04]\n",
      " [4.09670573e-05 4.25697698e-04 2.55094176e-04 1.66601647e-03]\n",
      " [1.38822711e-03 1.20585107e-11 1.74952545e-10 1.10718805e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.65033796e-04 2.04951527e-04 1.26505932e-02 3.45063027e-04]\n",
      " [1.90954459e-03 1.64565840e-02 1.00317475e-03 1.25806895e-03]\n",
      " [4.15529324e-03 4.89395674e-03 1.09471760e-02 7.18385512e-02]\n",
      " [5.94314824e-02 5.77192034e-02 3.42222648e-01 3.11487664e-02]\n",
      " [3.22806796e-04 2.47818143e-04 1.53319467e-04 1.30108395e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.32788405e-10 7.85979733e-04 3.95642139e-07 3.82976642e-07]\n",
      " [5.52540158e-05 4.28339399e-05 5.26003325e-05 2.10906070e-03]\n",
      " [6.73157482e-03 2.02029131e-05 8.62671463e-05 6.21242676e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.99300336e-02 2.63931272e-04 5.07918803e-01 3.15099729e-01]\n",
      " [2.61755429e-04 2.64121614e-04 8.66643084e-05 6.68203140e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.20452820e-11 7.07894338e-09 6.34988739e-06 5.02628470e-09]\n",
      " [8.43064359e-07 7.45707546e-13 1.10213620e-06 3.10725155e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.27074859e-01 1.02141239e-06 1.39594905e-04 2.73393453e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.94901832e-02 2.92589945e-02 9.11047804e-01 8.09835168e-02]\n",
      " [2.27613438e-04 2.07720746e-04 2.28857466e-04 8.42322369e-05]\n",
      " [2.60914924e-06 1.55561912e-04 8.71632807e-07 2.24562679e-06]\n",
      " [5.31377868e-05 4.82126645e-07 5.44618380e-06 6.05950099e-10]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.51851027e-08 1.98906773e-03 2.40061659e-06 1.10024773e-06]\n",
      " [1.88115474e-01 1.02204024e-01 6.02651352e-01 3.78720038e-02]\n",
      " [1.11166409e-01 8.88699447e-01 7.67316618e-03 1.97528447e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Successful Episodes: {}, Success Rate: {}\".format(successful_training_episodes, successful_training_episodes / total_training_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the Agent Play the Game with our Q-Table now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Episodes: 7346, Success Rate: 73.46%, Most Consecutive Successes: 28\n"
     ]
    }
   ],
   "source": [
    "successful_episodes = 0\n",
    "consecutive_successes = 0\n",
    "consecutive_successes_record = 0\n",
    "\n",
    "for episode in range(total_play_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while (not done):\n",
    "        action = np.argmax(qtable[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        step += 1\n",
    "    \n",
    "    if (verbosity > 0):\n",
    "        print(\"\\n{}Episode finished after {} steps. {}\".format(\"\\033[1m\\033[92m\" if has_won(state) else \"\", step, \"SOLVED!!!\\033[0m\" if has_won(state) else \"\"))\n",
    "        env.render()\n",
    "    \n",
    "    if (has_won(state)):\n",
    "        successful_episodes += 1\n",
    "        consecutive_successes += 1\n",
    "        \n",
    "        if (consecutive_successes > consecutive_successes_record):\n",
    "            consecutive_successes_record = consecutive_successes\n",
    "    else:\n",
    "        consecutive_successes = 0\n",
    "\n",
    "print(\"Successful Episodes: {}, Success Rate: {:3.2f}%, Most Consecutive Successes: {}\".format(successful_episodes, successful_episodes / total_play_episodes * 100, consecutive_successes_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successfull Q-Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[5.95622600e-03 7.69065614e-03 1.29795247e-02 7.66528198e-03]\n",
    "# [8.74553016e-03 8.65319714e-03 8.72166674e-03 1.37515984e-02]\n",
    "# [8.81988918e-03 8.74345842e-03 1.94510759e-02 9.85717695e-03]\n",
    "# [1.18944941e-02 2.06338083e-02 1.22873574e-02 1.19921980e-02]\n",
    "# [1.73632911e-02 1.93390616e-02 2.04212675e-02 2.57357832e-02]\n",
    "# [2.41697915e-02 2.13073046e-02 2.40035491e-02 3.65479906e-02]\n",
    "# [3.16741568e-02 3.78067173e-02 3.76115616e-02 4.02400611e-02]\n",
    "# [4.24389580e-02 4.10045871e-02 4.15269458e-02 4.12636846e-02]\n",
    "# [7.59993744e-03 5.07833173e-03 1.22899596e-02 7.67360036e-03]\n",
    "# [8.34331784e-03 8.76222400e-03 5.39142687e-03 1.26694183e-02]\n",
    "# [6.01496547e-03 7.06392586e-03 7.29510495e-03 1.47896524e-02]\n",
    "# [2.92117077e-03 1.38889310e-04 2.93924912e-03 1.90603093e-02]\n",
    "# [1.39507785e-02 9.62424956e-03 1.37226170e-02 1.96127576e-02]\n",
    "# [2.44461972e-02 1.23096432e-02 2.08778632e-02 3.22170692e-02]\n",
    "# [3.70801023e-02 4.18644622e-02 3.95616535e-02 4.24286478e-02]\n",
    "# [4.35499884e-02 4.23297843e-02 4.46662107e-02 4.28473276e-02]\n",
    "# [4.85419705e-03 3.57217550e-03 4.85763621e-03 9.74635854e-03]\n",
    "# [1.98756958e-03 1.66430775e-03 6.62345637e-03 7.28144773e-03]\n",
    "# [6.44407402e-03 1.03278578e-03 7.52992239e-04 6.25531512e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [9.86193984e-04 2.00869677e-03 2.81902240e-02 2.35364932e-05]\n",
    "# [7.02054894e-03 1.21763063e-03 1.17078996e-02 4.50884608e-02]\n",
    "# [4.43281104e-02 3.67499740e-02 1.01661659e-01 4.72615748e-02]\n",
    "# [4.60719795e-02 2.64849312e-01 5.62544578e-02 4.97058468e-02]\n",
    "# [2.79362830e-03 1.98764438e-03 2.43891370e-03 1.15725606e-02]\n",
    "# [3.04840849e-03 2.90759350e-03 4.83321013e-03 5.43260744e-04]\n",
    "# [2.80367515e-04 4.67973083e-04 1.75630797e-04 6.35989310e-03]\n",
    "# [3.52951718e-10 8.83436848e-08 9.47171686e-08 1.16648248e-03]\n",
    "# [7.92521972e-03 1.04370603e-04 4.51747821e-05 2.42738860e-05]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.96143817e-02 2.89523994e-02 2.38031086e-01 1.56725925e-02]\n",
    "# [5.49468161e-02 6.16086796e-02 1.27851490e-01 6.29401471e-02]\n",
    "# [4.69556282e-04 2.56309244e-03 5.23205406e-04 8.58511128e-03]\n",
    "# [5.93768821e-05 5.37151889e-05 3.12819175e-04 8.00110272e-03]\n",
    "# [2.11282516e-07 1.07990752e-09 1.76098119e-12 3.01649255e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.60768992e-04 8.78985324e-05 1.52134288e-02 1.93373189e-04]\n",
    "# [4.59251125e-05 3.95078011e-02 1.75938417e-04 1.57314475e-03]\n",
    "# [1.78047173e-03 9.98981879e-04 8.90742621e-03 1.41476471e-01]\n",
    "# [1.32630260e-01 6.90415944e-02 3.82889137e-01 1.22856861e-01]\n",
    "# [2.85430769e-03 3.65072096e-05 8.80596645e-05 2.29044875e-05]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.76082381e-08 1.66950115e-07 7.05655667e-06 1.05006010e-09]\n",
    "# [1.65966365e-06 9.78126002e-09 2.45898537e-05 1.88988008e-03]\n",
    "# [2.44260590e-02 3.13394824e-03 8.08105403e-05 2.84770156e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.39932865e-01 6.16363350e-02 7.71049639e-01 6.11065910e-02]\n",
    "# [1.53596945e-03 1.50959083e-06 2.17785434e-05 4.24965531e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [8.06125006e-09 1.25214214e-08 3.08602142e-04 2.81551190e-09]\n",
    "# [5.74118960e-05 1.27359187e-10 1.18211467e-10 2.50289029e-10]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [3.53403447e-04 3.87738188e-06 2.74009624e-02 1.37660814e-13]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [4.22635205e-03 1.65629656e-01 9.96055200e-01 2.53205922e-02]\n",
    "# [1.34464759e-03 1.25840735e-03 6.75619428e-05 9.90625422e-04]\n",
    "# [2.11688699e-07 1.40684221e-03 4.71952062e-06 1.43773972e-06]\n",
    "# [1.21293041e-03 1.03651765e-06 5.10582741e-07 8.32671350e-07]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [2.43436982e-05 2.71140378e-04 1.98503963e-03 7.10405904e-06]\n",
    "# [1.94681063e-01 5.16025328e-02 4.08468117e-01 1.01699777e-01]\n",
    "# [2.52934638e-02 5.67599658e-01 1.24092961e-01 1.16560682e-01]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
    "\n",
    "# 50000 Total Training Episodes\n",
    "# Successful Episodes: 173519, Success Rate: 0.347038\n",
    "#[[2.19549647e-02 2.17581636e-02 2.30287258e-02 2.24937164e-02]\n",
    "# [2.45807559e-02 5.52544062e-02 2.46254976e-02 2.46329635e-02]\n",
    "# [1.80817235e-02 5.25186945e-03 6.16385170e-02 3.09600180e-02]\n",
    "# [2.13607426e-02 2.17131811e-02 5.99103692e-02 5.90855967e-03]\n",
    "# [2.78457274e-02 2.61901375e-02 9.28584504e-02 2.39914521e-02]\n",
    "# [1.29879381e-02 3.21750413e-02 8.71393917e-02 4.48727752e-02]\n",
    "# [3.35216651e-02 6.78184242e-02 7.89616136e-02 3.35767698e-02]\n",
    "# [9.26536876e-02 7.78656889e-02 6.35847062e-02 1.79737562e-02]\n",
    "# [2.02929275e-02 6.45045557e-03 1.06724383e-02 1.93715831e-02]\n",
    "# [4.12128313e-03 7.38666325e-03 4.08388576e-03 4.43733583e-02]\n",
    "# [3.60697787e-03 1.29205325e-02 1.04713164e-02 4.73359132e-02]\n",
    "# [1.73564803e-03 2.51783576e-02 1.41936617e-02 9.96376526e-02]\n",
    "# [6.58863973e-03 7.25515823e-03 2.20116852e-02 1.07147163e-01]\n",
    "# [4.21295987e-02 1.59375440e-02 1.09785715e-01 1.57313863e-02]\n",
    "# [3.81608124e-02 1.00239886e-01 4.16836636e-02 4.15745474e-02]\n",
    "# [1.81237559e-01 6.80455036e-02 7.04207323e-02 4.27137727e-02]\n",
    "# [3.75462983e-03 2.19065396e-02 2.95004585e-03 2.04240932e-02]\n",
    "# [2.35770787e-03 1.91366962e-03 2.74283162e-03 2.68805321e-02]\n",
    "# [5.71696321e-03 1.05241601e-04 1.98866626e-03 6.93969544e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.45703332e-05 4.66474400e-04 4.34956941e-02 1.82898129e-03]\n",
    "# [3.07973626e-05 1.20112964e-03 6.72970133e-03 1.29056265e-01]\n",
    "# [9.34464502e-02 6.22878497e-02 2.11668769e-01 7.68678885e-02]\n",
    "# [6.06518658e-02 2.08638729e-01 5.90221476e-02 5.70317965e-02]\n",
    "# [2.99868394e-03 2.58302763e-03 2.44191313e-03 2.80395240e-03]\n",
    "# [1.59300161e-03 8.10871884e-04 5.90466341e-03 1.14998334e-03]\n",
    "# [9.27346078e-04 6.91661615e-05 1.29527120e-04 1.90656940e-04]\n",
    "# [1.71565364e-06 5.76475858e-03 1.04041533e-08 5.94300973e-06]\n",
    "# [4.27410206e-03 6.29733777e-05 6.58665242e-04 4.10884695e-03]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [8.17302026e-04 9.60726641e-05 1.90097543e-01 1.09390467e-03]\n",
    "# [1.98417021e-01 1.84594476e-01 9.04918861e-02 6.71138740e-02]\n",
    "# [8.85498599e-04 7.34252140e-04 1.01656590e-03 2.50798192e-03]\n",
    "# [6.50401657e-04 4.32939838e-05 2.10139070e-05 8.66158021e-04]\n",
    "# [7.66126822e-08 1.87352427e-06 1.34301352e-07 1.81904612e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [3.93634496e-05 2.08980141e-04 1.31998172e-01 3.88512261e-04]\n",
    "# [1.18037950e-02 1.13751715e-01 7.46358629e-03 8.18847292e-05]\n",
    "# [2.03424139e-03 6.20122733e-02 8.28225142e-04 2.18931287e-01]\n",
    "# [7.90015398e-02 2.54996415e-01 1.96793859e-01 1.98281355e-01]\n",
    "# [1.18657545e-03 1.88997003e-04 1.19223184e-04 2.23411630e-04]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [2.26554297e-14 5.14093791e-08 2.22594392e-05 2.11623546e-07]\n",
    "# [4.99648544e-07 4.82589823e-06 1.07479816e-04 5.84476291e-02]\n",
    "# [8.51017601e-02 1.33732556e-05 3.32945081e-05 1.65913528e-03]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.30441949e-03 3.48077319e-02 3.03640635e-01 3.44889420e-03]\n",
    "# [8.86931659e-04 1.65273540e-06 2.24100267e-04 3.41071476e-06]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.01902370e-08 2.88029741e-09 1.62752696e-06 5.66020061e-07]\n",
    "# [3.05971570e-04 2.47121172e-08 2.59046765e-10 2.87107183e-06]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.46571420e-06 1.45634456e-08 8.96206456e-02 1.52447329e-06]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [1.67568638e-01 1.96747464e-01 7.85298787e-01 1.54589243e-01]\n",
    "# [6.41463341e-04 8.37438530e-05 1.46721512e-04 1.32503269e-04]\n",
    "# [1.59225807e-05 7.20568974e-04 1.06490465e-05 5.79235727e-08]\n",
    "# [5.71985873e-05 3.03106331e-09 3.17554261e-07 1.46262145e-07]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
    "# [3.32610951e-05 1.82403373e-05 2.90993088e-05 5.95929718e-02]\n",
    "# [3.37421524e-01 1.24584589e-01 4.53448525e-01 7.84905081e-02]\n",
    "# [1.56422702e-01 9.59663405e-01 3.71583647e-02 3.34055793e-02]\n",
    "# [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
